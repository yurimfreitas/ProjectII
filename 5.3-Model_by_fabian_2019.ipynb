{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages gerais\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "\n",
    "# df_sales_ dataset\n",
    "df_sales_filtered_all = joblib.load('df_sales_filtered_all.pkl')\n",
    "df_sales_filtered_2018_2019 = joblib.load('df_sales_filtered_2018_2019.pkl')\n",
    "df_sales_filtered_2019 = joblib.load('df_sales_filtered_2019.pkl')\n",
    "df_sales_filtered_last_6_month = joblib.load('df_sales_filtered_last_6_month.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Create a date field based in Year and Week of the year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 But the date to make sense should be the last day in that specific week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Function to get the last day of the week using ISO calendar\n",
    "def get_last_day_of_iso_week(year, week):\n",
    "    first_day_of_year = datetime.datetime(year, 1, 4)  # 4th January is always in the first ISO week\n",
    "    first_monday_of_year = first_day_of_year - datetime.timedelta(days=first_day_of_year.weekday())\n",
    "    week_start_date = first_monday_of_year + datetime.timedelta(weeks=week-1)\n",
    "    return week_start_date + datetime.timedelta(days=6)\n",
    "\n",
    "# Applying function to DataFrame\n",
    "df_sales_filtered_2019['last_day_of_week'] = df_sales_filtered_2019.apply(\n",
    "    lambda x: get_last_day_of_iso_week(x['year'], x['week']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>year</th>\n",
       "      <th>week</th>\n",
       "      <th>sales</th>\n",
       "      <th>revenue</th>\n",
       "      <th>stock</th>\n",
       "      <th>price</th>\n",
       "      <th>last_day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52139</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>31.48</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-03-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52140</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>24.87</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-03-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52141</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>13.0</td>\n",
       "      <td>28.64</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52142</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>13</td>\n",
       "      <td>13.0</td>\n",
       "      <td>31.48</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52143</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>14</td>\n",
       "      <td>8.0</td>\n",
       "      <td>25.19</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52144</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>15</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.60</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-04-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52145</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>16</td>\n",
       "      <td>20.0</td>\n",
       "      <td>47.85</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-04-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52146</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33.04</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-04-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52147</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>18</td>\n",
       "      <td>13.0</td>\n",
       "      <td>33.36</td>\n",
       "      <td>47.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52148</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>19</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.75</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-05-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52149</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.60</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-05-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52150</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>21</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52151</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>22</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.66</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-06-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52152</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>23</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.81</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-06-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52153</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>24</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.02</td>\n",
       "      <td>51.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-06-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52154</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>25</td>\n",
       "      <td>25.0</td>\n",
       "      <td>55.09</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-06-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52155</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>26</td>\n",
       "      <td>11.0</td>\n",
       "      <td>28.02</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52156</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-07-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52157</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-07-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52158</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>29</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-07-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52159</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-07-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52160</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>31</td>\n",
       "      <td>17.0</td>\n",
       "      <td>41.24</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52161</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>32</td>\n",
       "      <td>24.0</td>\n",
       "      <td>52.89</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-08-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52162</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.90</td>\n",
       "      <td>42.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2019-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52163</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>34</td>\n",
       "      <td>8.0</td>\n",
       "      <td>28.74</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3.95</td>\n",
       "      <td>2019-08-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52164</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.66</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.95</td>\n",
       "      <td>2019-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52165</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>36</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.32</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.95</td>\n",
       "      <td>2019-09-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52166</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>37</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.94</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.95</td>\n",
       "      <td>2019-09-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52167</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>38</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.32</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.95</td>\n",
       "      <td>2019-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52168</th>\n",
       "      <td>S0097</td>\n",
       "      <td>P0704</td>\n",
       "      <td>2019</td>\n",
       "      <td>39</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.61</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.95</td>\n",
       "      <td>2019-09-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      store_id product_id  year  week  sales  revenue  stock  price  \\\n",
       "52139    S0097      P0704  2019    10   10.0    31.48   22.0   3.40   \n",
       "52140    S0097      P0704  2019    11   10.0    24.87   12.0   3.40   \n",
       "52141    S0097      P0704  2019    12   13.0    28.64   17.0   3.40   \n",
       "52142    S0097      P0704  2019    13   13.0    31.48   27.0   3.40   \n",
       "52143    S0097      P0704  2019    14    8.0    25.19   39.0   3.40   \n",
       "52144    S0097      P0704  2019    15    4.0    12.60   35.0   3.40   \n",
       "52145    S0097      P0704  2019    16   20.0    47.85   15.0   3.40   \n",
       "52146    S0097      P0704  2019    17   15.0    33.04   20.0   3.40   \n",
       "52147    S0097      P0704  2019    18   13.0    33.36   47.0   3.40   \n",
       "52148    S0097      P0704  2019    19    7.0    15.75   37.0   3.40   \n",
       "52149    S0097      P0704  2019    20    5.0    12.60   32.0   3.40   \n",
       "52150    S0097      P0704  2019    21    3.0     9.45   29.0   3.40   \n",
       "52151    S0097      P0704  2019    22    9.0    22.66   20.0   3.40   \n",
       "52152    S0097      P0704  2019    23    4.0     8.81   16.0   3.40   \n",
       "52153    S0097      P0704  2019    24    5.0    11.02   51.0   3.40   \n",
       "52154    S0097      P0704  2019    25   25.0    55.09   26.0   3.40   \n",
       "52155    S0097      P0704  2019    26   11.0    28.02   15.0   3.40   \n",
       "52156    S0097      P0704  2019    27    1.0     3.15   14.0   3.40   \n",
       "52157    S0097      P0704  2019    28    0.0     0.00   14.0   3.40   \n",
       "52158    S0097      P0704  2019    29    3.0     9.45   11.0   3.40   \n",
       "52159    S0097      P0704  2019    30    2.0     6.30    9.0   3.40   \n",
       "52160    S0097      P0704  2019    31   17.0    41.24   52.0   3.40   \n",
       "52161    S0097      P0704  2019    32   24.0    52.89   28.0   3.40   \n",
       "52162    S0097      P0704  2019    33    6.0    18.90   42.0   3.40   \n",
       "52163    S0097      P0704  2019    34    8.0    28.74   31.0   3.95   \n",
       "52164    S0097      P0704  2019    35    1.0     3.66   30.0   3.95   \n",
       "52165    S0097      P0704  2019    36    2.0     7.32   25.0   3.95   \n",
       "52166    S0097      P0704  2019    37    6.0    21.94   19.0   3.95   \n",
       "52167    S0097      P0704  2019    38    2.0     7.32   17.0   3.95   \n",
       "52168    S0097      P0704  2019    39    7.0    25.61   10.0   3.95   \n",
       "\n",
       "      last_day_of_week  \n",
       "52139       2019-03-10  \n",
       "52140       2019-03-17  \n",
       "52141       2019-03-24  \n",
       "52142       2019-03-31  \n",
       "52143       2019-04-07  \n",
       "52144       2019-04-14  \n",
       "52145       2019-04-21  \n",
       "52146       2019-04-28  \n",
       "52147       2019-05-05  \n",
       "52148       2019-05-12  \n",
       "52149       2019-05-19  \n",
       "52150       2019-05-26  \n",
       "52151       2019-06-02  \n",
       "52152       2019-06-09  \n",
       "52153       2019-06-16  \n",
       "52154       2019-06-23  \n",
       "52155       2019-06-30  \n",
       "52156       2019-07-07  \n",
       "52157       2019-07-14  \n",
       "52158       2019-07-21  \n",
       "52159       2019-07-28  \n",
       "52160       2019-08-04  \n",
       "52161       2019-08-11  \n",
       "52162       2019-08-18  \n",
       "52163       2019-08-25  \n",
       "52164       2019-09-01  \n",
       "52165       2019-09-08  \n",
       "52166       2019-09-15  \n",
       "52167       2019-09-22  \n",
       "52168       2019-09-29  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_filtered_2019[(df_sales_filtered_2019['store_id'] == 'S0097') & (df_sales_filtered_2019['product_id'] == 'P0704')].tail(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convert date to time series by set as index and sort that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'date' column as index and sort by date\n",
    "df_sales_filtered_2019.set_index('last_day_of_week', inplace=True)\n",
    "df_sales_filtered_2019.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 52906 entries, 2019-01-06 to 2019-09-29\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   store_id    52906 non-null  object \n",
      " 1   product_id  52906 non-null  object \n",
      " 2   year        52906 non-null  UInt32 \n",
      " 3   week        52906 non-null  UInt32 \n",
      " 4   sales       52906 non-null  float64\n",
      " 5   revenue     52906 non-null  float64\n",
      " 6   stock       52906 non-null  float64\n",
      " 7   price       52906 non-null  float64\n",
      "dtypes: UInt32(2), float64(4), object(2)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_sales_filtered_2019.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Prepare to apply ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'store_id' and 'product_id'\n",
    "grouped = df_sales_filtered_2019.groupby(['store_id', 'product_id'])\n",
    "\n",
    "# Create a DataFrame to store forecasts and a data frame to have those product/store with error\n",
    "df_forecasts = pd.DataFrame(columns=['store_id', 'product_id', 'forecast_week_1', 'forecast_week_2', 'forecast_week_3'])\n",
    "df_product_error = pd.DataFrame(columns=['store_id', 'product_id', 'error_message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Group by store_id and product_id\n",
    "grouped = df_sales_filtered_2019.groupby(['store_id', 'product_id'])\n",
    "\n",
    "# Function to perform grid search for ARIMA parameters\n",
    "def optimize_arima(series, p_values, d_value, q_values):\n",
    "    best_aic = float(\"inf\")\n",
    "    best_order = None\n",
    "    best_model = None\n",
    "    for p, q in product(p_values, q_values):\n",
    "        try:\n",
    "            model = ARIMA(series, order=(p, d_value, q))\n",
    "            model_fit = model.fit()\n",
    "            aic = model_fit.aic\n",
    "            if aic < best_aic:\n",
    "                best_aic = aic\n",
    "                best_order = (p, d_value, q)\n",
    "                best_model = model_fit\n",
    "        except:\n",
    "            continue\n",
    "    return best_order, best_model\n",
    "\n",
    "# Function to calculate MSE, RMSE, MAE, MAPE\n",
    "def calculate_metrics(actual, predicted):\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = mean_squared_error(actual, predicted, squared=False)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    mape = mean_absolute_percentage_error(actual, predicted)\n",
    "    return mse, rmse, mae, mape\n",
    "\n",
    "# Grid search parameters\n",
    "p_values = range(0, 3)\n",
    "d_value = 1\n",
    "q_values = range(0, 3)\n",
    "\n",
    "# DataFrames to store results\n",
    "df_forecasts = pd.DataFrame(columns=['store_id', 'product_id', 'forecast_week_1', 'forecast_week_2', 'forecast_week_3', 'ARIMA'])\n",
    "df_product_error = pd.DataFrame(columns=['store_id', 'product_id', 'error_message'])\n",
    "df_mse = pd.DataFrame(columns=['store_id', 'product_id', 'mse', 'rmse', 'mae', 'mape'])\n",
    "\n",
    "# Iterate over each group\n",
    "for (store_id, product_id), group in grouped:\n",
    "    # Reindex to ensure complete weekly intervals\n",
    "    group = group.asfreq('W-SUN', method='pad')\n",
    "    \n",
    "    # Ensure there are enough data points to fit the model\n",
    "    if len(group) < 2:\n",
    "        df_product_error = pd.concat([df_product_error, pd.DataFrame([{\n",
    "            'store_id': store_id,\n",
    "            'product_id': product_id,\n",
    "            'error_message': 'Not enough data points to fit ARIMA model'\n",
    "        }])], ignore_index=True)\n",
    "        continue\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")  # specify to ignore warning messages\n",
    "\n",
    "    # Fit ARIMA model\n",
    "    try:\n",
    "        best_order, best_model = optimize_arima(group['sales'], p_values, d_value, q_values)\n",
    "       \n",
    "        if best_model is not None:\n",
    "            # Forecast future sales (next 3 weeks)\n",
    "            forecast = best_model.forecast(steps=3)\n",
    "            \n",
    "            # Append the forecast to the DataFrame\n",
    "            df_forecasts = pd.concat([df_forecasts, pd.DataFrame([{\n",
    "                'store_id': store_id,\n",
    "                'product_id': product_id,\n",
    "                'forecast_week_1': forecast[0],\n",
    "                'forecast_week_2': forecast[1],\n",
    "                'forecast_week_3': forecast[2],\n",
    "                'ARIMA': best_order\n",
    "            }])], ignore_index=True)\n",
    "            \n",
    "            # Calculate metrics (assuming you have actual future sales for comparison)\n",
    "            # Replace 'actual_future_sales' with your actual sales data for the next 3 weeks\n",
    "            actual_future_sales = group['sales'][-3:]  # Adjust based on actual data availability\n",
    "            if len(actual_future_sales) == 3:\n",
    "                mse, rmse, mae, mape = calculate_metrics(actual_future_sales, forecast)\n",
    "                # Append the metrics to the DataFrame\n",
    "                df_mse = pd.concat([df_mse, pd.DataFrame([{\n",
    "                    'store_id': store_id,\n",
    "                    'product_id': product_id,\n",
    "                    'mse': mse,\n",
    "                    'rmse': rmse,\n",
    "                    'mae': mae,\n",
    "                    'mape': mape\n",
    "                }])], ignore_index=True)\n",
    "            else:\n",
    "                df_product_error = pd.concat([df_product_error, pd.DataFrame([{\n",
    "                    'store_id': store_id,\n",
    "                    'product_id': product_id,\n",
    "                    'error_message': 'Not enough actual future data to calculate metrics'\n",
    "                }])], ignore_index=True)\n",
    "        else:\n",
    "            df_product_error = pd.concat([df_product_error, pd.DataFrame([{\n",
    "                'store_id': store_id,\n",
    "                'product_id': product_id,\n",
    "                'error_message': 'Failed to find suitable ARIMA model'\n",
    "            }])], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting ARIMA for Store: {store_id}, Product: {product_id}\")\n",
    "        print(str(e))\n",
    "        df_product_error = pd.concat([df_product_error, pd.DataFrame([{\n",
    "            'store_id': store_id,\n",
    "            'product_id': product_id,\n",
    "            'error_message': str(e)\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "# Save the DataFrames to CSV files (or any other desired format)\n",
    "df_forecasts.to_csv('forecasts.csv', index=False)\n",
    "df_product_error.to_csv('product_errors.csv', index=False)\n",
    "df_mse.to_csv('2019_mse_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This input and merge the data predicted with the sales already done\n",
    "import numpy as np\n",
    "\n",
    "# Pivot the forecasts DataFrame\n",
    "df_forecasts_melted = df_forecasts.melt(id_vars=['store_id', 'product_id', 'ARIMA'], \n",
    "                                        value_vars=['forecast_week_1', 'forecast_week_2', 'forecast_week_3'], \n",
    "                                        var_name='week', value_name='forecast')\n",
    "\n",
    "# Extract the week number from the 'week' column\n",
    "df_forecasts_melted['week'] = df_forecasts_melted['week'].str.extract('(\\d+)').astype(int)\n",
    "df_forecasts_melted\n",
    "\n",
    "# Resetting index to ensure last_day_of_week is a regular column\n",
    "df_sales_filtered_2019.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# Step 1: Find the latest year and week number for each store_id and product_id\n",
    "latest_weeks = df_sales_filtered_2019.groupby(['store_id', 'product_id'])[['year', 'week']].max().reset_index()\n",
    "latest_weeks.columns = ['store_id', 'product_id', 'latest_year', 'latest_week']\n",
    "\n",
    "# Step 2: Merge the latest year and week numbers with the forecast DataFrame\n",
    "df_combined = df_forecasts_melted.merge(latest_weeks, on=['store_id', 'product_id'], how='left')\n",
    "\n",
    "# Step 3: Add the forecast weeks to the latest week numbers, adjusting for year transition\n",
    "def adjust_year_week(row):\n",
    "    new_week = row['latest_week'] + row['week']\n",
    "    new_year = row['latest_year']\n",
    "    while new_week > 52:  # Assuming 52 weeks in a year\n",
    "        new_week -= 52\n",
    "        new_year += 1\n",
    "    return new_year, new_week\n",
    "\n",
    "df_combined[['forecast_year', 'forecast_week']] = df_combined.apply(\n",
    "    lambda row: adjust_year_week(row), axis=1, result_type=\"expand\"\n",
    ")\n",
    "\n",
    "# Step 4: Calculate the forecast's last_day_of_week based on forecast_year and forecast_week\n",
    "def get_last_day_of_iso_week(year, week):\n",
    "    first_day_of_year = pd.Timestamp(year, 1, 4)\n",
    "    first_monday_of_year = first_day_of_year - pd.Timedelta(days=first_day_of_year.weekday())\n",
    "    week_start_date = first_monday_of_year + pd.Timedelta(weeks=week-1)\n",
    "    return week_start_date + pd.Timedelta(days=6)\n",
    "\n",
    "df_combined['last_day_of_week'] = df_combined.apply(\n",
    "    lambda row: get_last_day_of_iso_week(row['forecast_year'], row['forecast_week']), axis=1\n",
    ")\n",
    "\n",
    "# Step 5: Select and rename the necessary columns to match the desired format, using forecast as sales\n",
    "df_combined_final = df_combined[['store_id', 'product_id', 'forecast_year', 'forecast_week', 'forecast', 'last_day_of_week', 'ARIMA']]\n",
    "df_combined_final.columns = ['store_id', 'product_id', 'year', 'week', 'sales', 'last_day_of_week', 'ARIMA']\n",
    "\n",
    "# Step 6: Concatenate with the original sales DataFrame\n",
    "df_sales_final_arima = df_sales_filtered_2019[['store_id', 'product_id', 'year', 'week', 'sales', 'last_day_of_week']]\n",
    "df_sales_final_arima['ARIMA'] = np.nan  # Adding ARIMAX column with NaN for actual sales\n",
    "\n",
    "df_final_arima = pd.concat([df_sales_final_arima, df_combined_final], ignore_index=True).sort_values(by=['store_id', 'product_id', 'year', 'week'])\n",
    "\n",
    "# Display the final DataFrame\n",
    "df_final_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
